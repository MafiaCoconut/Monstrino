---
id: adr-002-processing-state-vs-kafka
title: ADR-002 â€” DB Processing State vs Kafka-Based Pipeline
sidebar_label: ADR-002 Processing State vs Kafka
---

> **Status:** Accepted  
> **Scope:** Data ingestion & processing pipeline  
> **Audience:** Engineering / Architecture review

---

## Context

The system ingests structured data from external sources (releases, characters, series, pets).
This ingestion process is **not atomic** and may fail partially due to:

- malformed external data,
- missing or inconsistent fields,
- schema evolution across sources,
- parser or resolver logic errors.

A core requirement is **observability and recoverability**:
- failed records must be inspectable,
- invalid data must never pollute the canonical domain tables,
- partial failures must not block the entire ingestion pipeline,
- retries and manual inspection must be possible.

At this stage, ingestion correctness and debuggability were prioritized over throughput.

---

## Decision

Use a **database-driven processing state machine** instead of a Kafka-based pipeline.

Each parsed record is persisted in parsed tables and transitions through explicit states,
stored directly in the database:

- initialized
- processed
- with_errors

Processing services operate by selecting records based on their current state and updating
that state transactionally after each step.

Invalid or partially processed data is **isolated by state**, not removed or hidden.

---

## Processing Model

```text
External Source
      â†“
Parsed Tables
      â†“
processing_state = initialized
      â†“
Importer / Processor
      â†“
processed | with_errors
```

State transitions are explicit, queryable, and auditable.

---

## Alternatives Considered

### 1. Kafka-Based Processing Pipeline

Parsed records are published as messages and consumed by downstream services.

:::danger Rejected
- Message processing is sequential per partition; a single failing message blocks progress
- Error handling and retries add significant complexity
- Failed messages require separate dead-letter queues
- Inspecting and manually correcting individual records is cumbersome
- Debugging requires reconstructing state from logs and offsets
:::

Kafka optimizes **throughput**, while the problem domain required **traceability and control**.

---

### 2. In-Memory or Queue-Based Retry Logic

Handle retries and failures inside the processing service.

:::danger Rejected
- State is lost on restarts
- No durable audit trail
- Hard to inspect or intervene manually
- Difficult to reason about system state after partial failures
:::

---

## Consequences

### âœ… Positive

- Full visibility into processing progress via database queries
- Failed records are preserved for inspection and correction
- No risk of invalid data reaching canonical tables
- Simple retry logic based on state transitions
- Deterministic and debuggable ingestion behavior

---

### âš ï¸ Negative

- Lower throughput compared to streaming pipelines
- Requires careful indexing and state queries
- Processing logic must explicitly manage state transitions
- Less suitable for real-time, high-volume ingestion

---

### ðŸŽ¯ Accepted Trade-off

The system intentionally prioritizes:

> **correctness, traceability, and debuggability**  
> over  
> **maximum ingestion throughput**

This decision is aligned with the early-stage focus on data quality and domain integrity.

---

## Notes for Reviewers

:::note
Kafka was intentionally deferred, not rejected permanently.

If ingestion volume or real-time requirements increase significantly,
the existing processing state model can be migrated to an event-driven pipeline,
using the current states as semantic equivalents of processing stages.
:::